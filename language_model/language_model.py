"""
language_model.py

This module initializes a language model to respond to the users input  
"""

import logging
from transformers import AutoModelForCausalLM, AutoTokenizer

# Not sustainable but using this to suppresses console logs
logging.getLogger('transformers').setLevel(logging.ERROR)

# Constants
MODEL_NAME = "microsoft/DialoGPT-medium"

# Load the pre-trained model and tokenizer
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side="left")

def bot_response(translated_text):
    """
    Utilizes the language model to generate a chat like response to the user input

    Parameters:
    translated_text (string):  Input of the user translated to English.

    Returns:
    An English reply to the user input as generated by the language model
    """

    # take user input
    text = translated_text

    # encode the input and add end of string token
    input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors="pt")
    # concatenate new user input with chat history (if there is)
    # bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids
    bot_input_ids = input_ids

    # generate a bot response
    chat_history_ids = model.generate(
        bot_input_ids,
        max_length=1000,
        do_sample=True,
        top_k=100,
        temperature=0.63,
        pad_token_id=tokenizer.eos_token_id
    )

    #print the output
    output_text = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
    
    print(f"Bot: {output_text}")

    return output_text