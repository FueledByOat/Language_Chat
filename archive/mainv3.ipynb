{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install vosk\n",
    "%pip install pyaudio\n",
    "%pip install transformers\n",
    "%pip install wave\n",
    "%pip install googletrans==4.0.0-rc1\n",
    "%pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "import wave\n",
    "import sys\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "import re\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Records Audio Sample\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 3\n",
    "\n",
    "with wave.open('human_input.wav', 'wb') as wf:\n",
    "    p = pyaudio.PyAudio()\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True)\n",
    "\n",
    "    print('Recording...')\n",
    "    for _ in range(0, RATE // CHUNK * RECORD_SECONDS):\n",
    "        wf.writeframes(stream.read(CHUNK))\n",
    "    print('Done')\n",
    "\n",
    "    stream.close()\n",
    "    p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play Audio Section\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print(f'Plays a wave file. Usage: {sys.argv[0]} filename.wav')\n",
    "    sys.exit(-1)\n",
    "\n",
    "with wave.open(\"human_input.wav\", 'rb') as wf:\n",
    "    # Instantiate PyAudio and initialize PortAudio system resources (1)\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # Open stream (2)\n",
    "    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                    channels=wf.getnchannels(),\n",
    "                    rate=wf.getframerate(),\n",
    "                    output=True)\n",
    "\n",
    "    # Play samples from the wave file (3)\n",
    "    while len(data := wf.readframes(CHUNK)):  # Requires Python 3.8+ for :=\n",
    "        stream.write(data)\n",
    "\n",
    "    # Close stream (4)\n",
    "    stream.close()\n",
    "\n",
    "    # Release PortAudio system resources (5)\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\" : \"您好\"\n",
      "}\n",
      "Recognized text: 您好\n",
      "Final recognized text: 您好\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Chinese Speech Recognition with Vosk\n",
    "\n",
    "# Path to the downloaded Vosk model\n",
    "model_path = \"vosk-model-small-cn-0.22\"\n",
    "\n",
    "# Path to the input WAV file\n",
    "wav_file_path = \"human_input.wav\"\n",
    "\n",
    "# Load the Vosk model\n",
    "model = Model(model_path)\n",
    "\n",
    "# Open the WAV file\n",
    "with wave.open(wav_file_path, \"rb\") as wf:\n",
    "    # Check if the audio file has the correct parameters\n",
    "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() not in [8000, 16000, 32000, 44100, 48000]:\n",
    "        # print(\"Audio file must be WAV format mono PCM.\")\n",
    "        raise Exception(\"Audio file must be WAV format mono PCM.\")\n",
    "\n",
    "    # Create a Kaldi recognizer with the model and the sample rate\n",
    "    recognizer = KaldiRecognizer(model, wf.getframerate())\n",
    "\n",
    "    # Read the audio data and transcribe it\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = recognizer.Result()\n",
    "            # print(result)\n",
    "            text = json.loads(result).get('text', '')\n",
    "            # print(f\"Recognized text: {text}\")\n",
    "\n",
    "    # Final result\n",
    "    final_result = recognizer.FinalResult()\n",
    "    final_result_test = json.loads(final_result).get('text', '')\n",
    "\n",
    "    if final_result_test != \"\":\n",
    "        chinese_input_text = json.loads(final_result).get('text', '')\n",
    "        chinese_input_text = re.sub(r\"\\s+\", \"\", chinese_input_text, flags=re.UNICODE)\n",
    "    else:\n",
    "        chinese_input_text = re.sub(r\"\\s+\", \"\", text, flags=re.UNICODE)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"Final recognized text: {chinese_input_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Translate from Chinese to English \n",
    "\n",
    "\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Translate from Chinese to English\n",
    "translated = translator.translate(chinese_input_text, src='zh-cn', dest='en')\n",
    "print(translated.text)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hello, how are you?\n",
      "Output: Good morning everyone!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Hello, how are you?\"\n",
    "output_text = generate_response(input_text)\n",
    "output_text = output_text.strip(input_text)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大家，早安！\n"
     ]
    }
   ],
   "source": [
    "# Step 4:  Translate back to Chinese\n",
    "\n",
    "# Translate from English to Chinese\n",
    "translated_to_zh = translator.translate(output_text, src='en', dest='zh-cn')\n",
    "print(translated_to_zh.text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Translate reply back to Chinese \n",
    "\n",
    "# Initialize the TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set properties before adding anything to speak\n",
    "engine.setProperty('rate', 125)  # Speed percent (can go over 100)\n",
    "engine.setProperty('volume', 1)  # Volume 0-1\n",
    "\n",
    "# List available voices and set the voice to a Chinese one if available\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    # print(f\"Voice: {voice.name}, ID: {voice.id}, Languages: {voice.languages}\")\n",
    "    if 'ZH' in voice.languages or 'ZH-CN' in voice.id:\n",
    "        engine.setProperty('voice', voice.id)\n",
    "        break\n",
    "\n",
    "# Text to be spoken\n",
    "text_to_speak = translated_to_zh.text\n",
    "\n",
    "# Speak the text\n",
    "engine.say(text_to_speak)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play Audio Section\n",
    "\n",
    "\n",
    "\n",
    "with wave.open(\"chinese_output.mp3\", 'rb') as wf:\n",
    "    # Instantiate PyAudio and initialize PortAudio system resources (1)\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # Open stream (2)\n",
    "    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                    channels=wf.getnchannels(),\n",
    "                    rate=wf.getframerate(),\n",
    "                    output=True)\n",
    "\n",
    "    # Play samples from the wave file (3)\n",
    "    while len(data := wf.readframes(CHUNK)):  # Requires Python 3.8+ for :=\n",
    "        stream.write(data)\n",
    "\n",
    "    # Close stream (4)\n",
    "    stream.close()\n",
    "\n",
    "    # Release PortAudio system resources (5)\n",
    "    p.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
